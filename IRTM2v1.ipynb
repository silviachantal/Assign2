{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import re \n",
    "import preprocessor as p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "    data = pd.read_csv(path, sep='\\t', on_bad_lines='skip', names=['DATE', 'ID', 'HANDLE', 'NAME', 'DATA'])\n",
    "    #drop duplicates \n",
    "    data = data.drop_duplicates(subset=None, keep='first', inplace=False)  \n",
    "    mydata = data[['ID', 'DATA']].copy()\n",
    "    return mydata\n",
    "    textt = mydata['DATA']\n",
    "    return textt\n",
    "    #print(mydata.iloc[:3])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ID                                               DATA\n",
      "0  965734992633565184  @knakatani @ChikonJugular @joofford @SteveBlog...\n",
      "1  965706998946893824     @FischerKurt Lady, whatÂ´s a tumor? #KippCharts\n",
      "2  965695626150326273  @Kings_of_Metal Ohne Diagnoseverdacht ist es n...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mydata = read_csv(\"/home/silvia/Documents/IRTM/tweets.csv\")\n",
    "print(mydata.iloc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "# Use twitter preprocessor to remove mentions, links, hashtags, and emojis\n",
    "    text = p.clean(text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\[NEWLINE\\]', \" \", text)\n",
    "    text = re.sub(r'\\[TAB\\]', \" \", text)# Lower case\n",
    "    text = text.lower()\n",
    "    # Expand common contractions\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"cannot\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "\n",
    "\n",
    "    tokens = text.split()\n",
    "    # Stem and tokenize\n",
    "    #ps = PorterStemmer()\n",
    "    #text = [ps.stem(word.strip(string.punctuation)) for word in text.split()]\n",
    "    # Return tokens\n",
    "    return [i for i in tokens if i !='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(df):\n",
    "    # Progress bar for text cleaning and tokenization\n",
    "    #tqdm.pandas(desc='Loading Database')\n",
    "    # Cleans and tokenizes\n",
    "    df['TOKENIZED'] = [clean_and_tokenize(text) for text in df['DATA']]\n",
    "\n",
    "\n",
    "    # Creates a list of tuples maping each token to a tweet id\n",
    "    normalized_toks = df.TOKENIZED.tolist()\n",
    "    IDs = df.ID.tolist()\n",
    "    ds = list(zip(normalized_toks, IDs))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tokenize_df(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_idx(ids):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for words, doc in ds:\n",
    "        for word in words:\n",
    "            inverted_index[word].append(doc)\n",
    "    vocab = sorted(inverted_index.keys())\n",
    "    return vocab, inverted_index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, idx = create_idx(ds)\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import *\n",
    "\n",
    "def permutator(vocab):\n",
    "    #we create a dictionary to store terms of vocabulary as keys and their permutations as values\n",
    "    permutations = defaultdict(list)\n",
    "    #we iterate over each term in our vocabulary\n",
    "    for term in vocab:\n",
    "        #we make sure to exclude punctuations and numbers --> maybe change?\n",
    "        if term.isalpha() == True:\n",
    "            #in case the term is one letter or multiple same letters we do not need permutations\n",
    "            if len(term) == 1 or term == term[0] * len(term):\n",
    "                permutations[term].append(term)\n",
    "            #in all other cases we first add the end of string symbol $ to then perform permutations\n",
    "            else:\n",
    "                termdollar = term + \"$\"\n",
    "                \n",
    "                #now we iterate over each character in each term \n",
    "                for char in range(len(termdollar)):\n",
    "                    #and we store iteratively the first char followed by next in the term -1\n",
    "                    allpermuts = termdollar[char:] + termdollar[:char]\n",
    "                    #we append to our dictionary all the permutations of a specific term (key) as list (value)\n",
    "                    permutations[term].append(allpermuts)\n",
    "                \n",
    "    return permutations     \n",
    "\n",
    "# this function transforms the query input of user by adding $ symbol and the kleene star\n",
    "# at the end of the word\n",
    "def query_convertor(querywd):\n",
    "    size = len(querywd)\n",
    "    # we add the end of string symbol \"$\" to the query word \n",
    "    querywd = querywd + \"$\"\n",
    "    # we find where the index of the kleene star is \n",
    "    index = querywd.find(\"*\")\n",
    "    # we transform the query word in the shape of what originally comes after the kleene start \n",
    "    # + what originally comes before, followed by the start itself (end of string)\n",
    "    querywd = querywd[index+1:size+1] + querywd[0:index]\n",
    "    return querywd\n",
    "\n",
    "# this function takes a queryword and a vocabulary as input \n",
    "# this function converts the query word according to the converter and \n",
    "# as output we get a list of terms of the vocabulary whose permutation match with the query\n",
    "def queryvocabmatcher(queryword, vocabu):\n",
    "    queryword = query_convertor(queryword)\n",
    "    #container to store where permutation or bigram of a term matches the query \n",
    "    matches = []\n",
    "\n",
    "    # first we unwrap the terms and their lists of permutations or bigrams\n",
    "    for key, val in vocabu.items():\n",
    "        # from the permutations or bigrams we check whether there is some whose \n",
    "        # start matches the query word\n",
    "        for word in val:\n",
    "            if word.startswith(queryword):\n",
    "                matches.append(key)\n",
    "    return matches\n",
    "\n",
    "# we instantiate a class TreeNode so that we can recursively store and \n",
    "# retrieve the different permutations of each term in our vocabulary. We create such \n",
    "# a tree to store more efficiently\n",
    "class TreeNode():\n",
    "    def __init__(self) -> None:\n",
    "        # in the children attribute we will keep track of all children nodes \n",
    "        self.children : Dict[str, TreeNode] = defaultdict(TreeNode)\n",
    "        # here we'll store the terms matching the query\n",
    "        self.terms = []\n",
    "\n",
    "    @staticmethod \n",
    "    def construct_from_vocab( words: List[str]): # -> TreeNode:\n",
    "        # we initialize our Tree object by setting the root node\n",
    "        root = TreeNode()\n",
    "        # we create all the permutations\n",
    "        permuated_dict = permutator(words)\n",
    "        for term, permuations in permuated_dict.items():\n",
    "            for permutation in permuations:\n",
    "                root.insert(permutation, term)\n",
    "        return root\n",
    "\n",
    "    def insert(self, permutation: str, term: str):\n",
    "        # each time we reach the end of the bigrams we add the final matching term\n",
    "        # to the list of all terms matching\n",
    "        if len(permutation) == 0:\n",
    "            self.terms.append(term)\n",
    "        # whereas if we (still) have permutations we construct a new branch \n",
    "        else:\n",
    "            # e.g. dog$\n",
    "            ch = permutation[0] # e.g. \"d\"\n",
    "            tail = permutation[1:] # e.g. \"og$\"\"\n",
    "            child = self.children[ch]\n",
    "            child.insert(tail,term)\n",
    "    \n",
    "    def query(self, query:str):\n",
    "        # depending on the query we will return the nodes \n",
    "        query = query_convertor(query)\n",
    "        return self.find_node_with_prefix(query)\n",
    "    \n",
    "    def find_node_with_prefix(self, query: str): # $do\n",
    "        # we want to find recursively the sutree where the edges lead \n",
    "        # to the subtree: the word on the edges leading to subtree is query\n",
    "        if len(query) == 0: \n",
    "            return self.collect_match()\n",
    "        else:\n",
    "        # if the length is not zero we descend recursively  \n",
    "            ch = query[0] # d\n",
    "            tail = query[1:] # og$\n",
    "            child = self.children[ch] \n",
    "            return child.find_node_with_prefix(tail)\n",
    "    \n",
    "    def collect_match(self):\n",
    "        # we visit all the child nodes recursively in subtree \n",
    "        # and collect all terms stored in subtree\n",
    "        result = []\n",
    "        result += self.terms\n",
    "        for child in self.children.values():\n",
    "            result += child.collect_match()\n",
    "        return result\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeperm = TreeNode.construct_from_vocab(vocab)\n",
    "permutations = permutator(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(treeperm.query(\"zeit*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'permutations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#setresults = set(queryvocabmatcher(\"zeit*\", permutations)) #print(len(setresults))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m queryoneperm \u001b[39m=\u001b[39m queryvocabmatcher(\u001b[39m\"\u001b[39m\u001b[39mzeit*\u001b[39m\u001b[39m\"\u001b[39m, permutations)\n\u001b[1;32m      3\u001b[0m querytwoperm \u001b[39m=\u001b[39m queryvocabmatcher(\u001b[39m\"\u001b[39m\u001b[39m*zeit\u001b[39m\u001b[39m\"\u001b[39m, permutations)\n\u001b[1;32m      4\u001b[0m querythreeperm \u001b[39m=\u001b[39m queryvocabmatcher(\u001b[39m\"\u001b[39m\u001b[39mze*st\u001b[39m\u001b[39m\"\u001b[39m, permutations)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'permutations' is not defined"
     ]
    }
   ],
   "source": [
    "#setresults = set(queryvocabmatcher(\"zeit*\", permutations)) #print(len(setresults))\n",
    "queryoneperm = queryvocabmatcher(\"zeit*\", permutations)\n",
    "querytwoperm = queryvocabmatcher(\"*zeit\", permutations)\n",
    "querythreeperm = queryvocabmatcher(\"ze*st\", permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"This is result for query zeit* :\", queryoneperm)\n",
    "# print(\"This is result for query *zeit :\", querytwoperm)\n",
    "# print(\"This is result for query ze*st :\", querythreeperm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryonetreeperm = treeperm.query(\"zeit*\")\n",
    "querytwotreeperm = treeperm.query(\"*zeit\")\n",
    "querythreetreeperm = treeperm.query(\"ze*st\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"This is result for query zeit* :\", queryonetreeperm)\n",
    "# print(\"This is result for query *zeit :\", querytwotreeperm)\n",
    "# print(\"This is result for query ze*st :\", querythreetreeperm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here bigram index \n",
    "def bigramidx(vocab):\n",
    "    #we create a dictionary to store terms of vocabulary as keys and the bigrams as values\n",
    "    bigrams = defaultdict(list)\n",
    "    #we iterate over each term in our vocabulary\n",
    "    for term in vocab:\n",
    "        #we make sure to exclude punctuations and numbers \n",
    "        if term.isalpha() == True:\n",
    "            termdollar = \"$\" + term + \"$\"\n",
    "            for i in range(len(termdollar)-1):\n",
    "                termbigrs = termdollar[i:i+2]\n",
    "                #termsbigrs = nltk.ngrams(termdollar, 2) #nltk version\n",
    "                bigrams[term].append(termbigrs)\n",
    "                                               \n",
    "    return bigrams\n",
    "\n",
    "# this changes from the permut convertor because before returning \n",
    "# we split into bigrams \n",
    "def query_convertorbigr(querywd):\n",
    "    size = len(querywd)\n",
    "    # we add the end of string symbol \"$\" to the query word \n",
    "    querywd = querywd + \"$\"\n",
    "    # we find where the index of the kleene star is \n",
    "    index = querywd.find(\"*\")\n",
    "    # we transform the query word in the shape of what originally comes after the kleene start \n",
    "    # + what originally comes before, followed by the start itself (end of string)\n",
    "    querywd = querywd[index+1:size+1] + querywd[0:index]\n",
    "    bigrqueryres = []\n",
    "    for i in range(len(querywd)-1):\n",
    "        bigrqueryres.append(querywd[i:i+2])\n",
    "        \n",
    "\n",
    "    return bigrqueryres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tree is similar to the one for the permuterm but we get a different query funct\n",
    "# specific for queries where we want bigrams as output and then we will look for intersections\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self) -> None:\n",
    "        self.children: Dict[str, TreeNode] = defaultdict(TreeNode)\n",
    "        self.terms = []\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_from_bigramidx(words: List[str]):  # -> TreeNode:\n",
    "        # we initialize our Tree object by setting the root node\n",
    "        root = TreeNode()\n",
    "        # we create all the permutations\n",
    "        bigrm_dict = bigramidx(words)\n",
    "        for term, bigrams in bigrm_dict.items():\n",
    "            # we add a bigram to a new root\n",
    "            for bigram in bigrams:\n",
    "                root.insert(bigram, term)\n",
    "        return root\n",
    "\n",
    "    def insert(self, bigram: str, term: str):\n",
    "\n",
    "        # each time we reach the end of the bigrams we add the final matching term\n",
    "        # to the list of all terms matching\n",
    "        if len(bigram) == 0:\n",
    "            self.terms.append(term)\n",
    "        else:\n",
    "            ch = bigram[0]\n",
    "            tail = bigram[1:]\n",
    "            child = self.children[ch]\n",
    "            child.insert(tail, term)\n",
    "\n",
    "    def querybigram(self, query: str):\n",
    "        query = query_convertorbigr(query)\n",
    "        #print(query)\n",
    "        q_results = []\n",
    "        for bigram in query:\n",
    "            r = self.find_node_with_prefix(bigram)\n",
    "            r = set(r)\n",
    "            #print(r)\n",
    "            q_results.append(r)\n",
    "\n",
    "        result = set.intersection(*q_results)\n",
    "        return result\n",
    "\n",
    "    def find_node_with_prefix(self, query: str):  # do\n",
    "        if len(query) == 0:\n",
    "            return self.collect_match()\n",
    "        else:\n",
    "            ch = query[0]  # d\n",
    "            tail = query[1:]  # o\n",
    "            return self.children[ch].find_node_with_prefix(tail)\n",
    "\n",
    "    def collect_match(self):\n",
    "        result = []\n",
    "        result += self.terms\n",
    "        for child in self.children.values():\n",
    "            result += child.collect_match()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebigr = TreeNode.construct_from_bigramidx(vocab)\n",
    "bigrams = bigramidx(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zeitgleich', 'zeit', 'zeitgeschichtlichen', 'zeitlupe', 'zeitumstellung', 'zeitgenossen', 'zeitalter', 'zeitnah', 'zeitpkt', 'zeitungscallcenter', 'zeitzeugen', 'zeitintensiv', 'zeitgenssische', 'zeitlang', 'zeitstein', 'zeitzonen', 'zeitrechnung', 'zeitschinder', 'zeitungen', 'zeitmanagement', 'zeitroboter', 'zeitabeit', 'zerreit', 'zeitschrift', 'zeitung', 'zerisssenheit', 'zeitbombe', 'zeitschleife', 'zeitgem', 'zeitgeme', 'zeitrhythmus', 'zeitraffertrack', 'zeitgeist', 'zwischenzeitlich', 'zeitungsbeitrgen', 'zeitlich', 'zeitweilig', 'zeitverschwendung', 'zeitig', 'zeitschriften', 'zeitzone', 'zwischenzeit', 'zeitraum', 'zugzeit', 'zeitgrnden', 'zeitlos', 'zeitgeschichte', 'zeitungspapier', 'zeitweise', 'zeitschaltung', 'zurzeit', 'zeitschriftenportals', 'zeitungskommentare', 'zeitpunkt', 'zeitreisender', 'zeitdruck', 'zeiten', 'zeitpunk', 'zeitgemsse', 'zeitvertreib', 'zeitalters', 'zeitliche', 'zeitwchter', 'zwischenmahlzeit', 'zeitlinie', 'zeitmaschine', 'zeitgenssischen', 'zeittiming', 'zeitigt', 'zeitgemes', 'zeitspanne', 'zeitenwende', 'zeitlinieebem', 'zeitzeuge', 'zeiterl', 'zeitnahmr', 'zeitiger', 'zeitgenssisches', 'zeitnahe', 'zeitfenster', 'zeitlichen', 'zeitgeistlicher'}\n"
     ]
    }
   ],
   "source": [
    "print(treebigr.querybigram(\"zeit*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'realschulzeit', 'regenzeit', 'teilzeit', 'studienzeit', 'zeit', 'elternzeit', 'prophezeit', 'spielzeit', 'nachkriegszeit', 'zeitpkt', 'diamanthochzeit', 'irrsinnszeit', 'hochzeitsangst', 'passierenzeit', 'nachspielzeit', 'drogenzeit', 'konzernzeitschrift', 'teenagerzeit', 'redezeit', 'tageszeit', 'frhzeit', 'abizeit', 'zeitmanagement', 'pollenzeit', 'wartelistenzeit', 'infostandzeit', 'teeniezeit', 'winterzeit', 'praktikumszeit', 'fastenzeit', 'zeitabeit', 'zerreit', 'erntezeit', 'zeitschrift', 'zerisssenheit', 'regelstudienzeit', 'herzensangelegenheit', 'schlafenszeit', 'sendezeit', 'neuzeit', 'geschichtenzeit', 'zeitgeist', 'durchschnittsspielzeit', 'bronzezeit', 'kinderzeit', 'vorlaufzeit', 'familienzeit', 'lebenszeit', 'grundzeit', 'wartezeit', 'halbzeit', 'derzeit', 'zwischenzeit', 'zugzeit', 'allzeit', 'vorlesungszeit', 'mahlzeit', 'berlebenszeit', 'sommerzeit', 'urzeit', 'lebzeit', 'horrorzeit', 'weihnachtszeit', 'jugendfreizeit', 'retentionszeit', 'amtszeit', 'traumhochzeit', 'zurzeit', 'halbwertszeit', 'trauerfestspielzeit', 'hauptverkehrszeit', 'kaiserzeit', 'zeitpunkt', 'fahrtzeit', 'fliederzeit', 'vollzeit', 'nazizeit', 'frhlingszeit', 'knuddelzeit', 'auszeit', 'arbeitszeit', 'tatzeit', 'langzeit', 'mittagszeit', 'allergiezeit', 'lokalzeit', 'freizeit', 'besuchszeit', 'bungszeit', 'wahrheitskonzept', 'karnevalszeit', 'zwischenmahlzeit', 'familienfreizeit', 'echtzeit', 'jederzeit', 'verbotszeit', 'pausenzeit', 'piratenzeit', 'seinerzeit', 'zeitigt', 'steizeit', 'uhrzeit', 'gartenzeit', 'langezeit', 'schulzeit', 'inkubationszeit', 'bedenkzeit', 'pollenflugzeit', 'aktivierungszeit', 'langzeitstudent', 'steinzeit', 'jahreszeit', 'faschingszeit', 'doppelhochzeit', 'osterzeit', 'fuckboyzeit', 'rekordzeit', 'eiszeit', 'teilzeitarbeit', 'hochzeit', 'haltbarkeitszeit', 'vorzeit'}\n"
     ]
    }
   ],
   "source": [
    "print(treebigr.querybigram(\"*zeit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zeichnest', 'zeigst', 'zerfrisst', 'zeitgeist', 'zerstrt', 'zerstckelt', 'zerstrungswut', 'zeigtest', 'zerstrst'}\n"
     ]
    }
   ],
   "source": [
    "print(treebigr.querybigram(\"ze*st\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf604985f5bdaaa347674bcbbe6b60a804d1f4095da3e753738392c99c6efde7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
